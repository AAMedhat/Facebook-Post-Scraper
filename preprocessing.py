import subprocess

# Install nltk using pip
subprocess.call(['pip', 'install', 'nltk'])

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.stem.isri import ISRIStemmer
import string

df = pd.read_csv('/home/combined_data.csv')
# print(df.head(10))

df = df[['Comment']]

# Print the first 10 rows of the modified DataFrame
# print(df.head(10))
# print(df.duplicated().sum())

# print("Before removing duplicates: ", df.shape)

# Remove duplicates
df = df.drop_duplicates()
df = df.dropna()

# Check the shape after removing duplicates
# print("After removing duplicates: ", df.shape)

df = df.dropna(subset=['Comment'])

# print("After removing nan: ", df.shape)

mentions = []
for Comment in df['Comment']:
    if Comment is not None:
        mentions.extend(re.findall(r'@\w+', Comment))

# Remove all mentions from the dataset
df['Comment'] = df['Comment'].apply(lambda Comment: re.sub(r'@\w+', '', Comment) if Comment is not None else Comment)
# print(mentions)
df.head(10)

def is_english(text):
    if text and re.match('^[A-Za-z0-9 .,!?\-\'"]*$', text):
        return None  # Return None for English text
    return text  # Keep the original text for non-English

df['Comment'] = df['Comment'].apply(is_english)


#remove English
english = ["A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z",
          "a","b","c","d","e","f","g","h","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z"]
for eng in english:
    df['Comment'] = df['Comment'].str.replace(eng, ' ')

# print(df.head(10))

# Regular expression pattern for identifying most URLs
url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

# Replace URLs with an empty string in the specific column
df['Comment'] = df['Comment'].str.replace(url_pattern, '', regex=True)
df.head(10)


#check emails

def check_emails_in_reviews(df):
    # Regular expression pattern for detecting email addresses
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

    # List to store rows where email addresses are found
    rows_with_emails = []

    # Iterate over each Comment in the DataFrame
    for index, Comment in df['Comment'].items():
        # Check if the Comment is a string and contains an email
        if isinstance(Comment, str) and re.search(email_pattern, Comment):
            rows_with_emails.append(index)

    return rows_with_emails

rows_with_emails = check_emails_in_reviews(df)
# print(rows_with_emails)

def check_links(dataframe):
    # Regular expression pattern for identifying a link
    link_pattern = r'https?://\S+A'
    rows_with_link= []

    # Function to apply to each comment to check for a link
    for index, Comment in df['Comment'].items():
      # Check if the Comment is a string and contains an email
      if isinstance(Comment, str) and re.search(link_pattern, Comment):
        rows_with_link.append(index)

    return rows_with_link

rows_with_link = check_links(df)
# print(rows_with_link)


# Regular expression pattern for identifying most URLs
url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

# Replace URLs with an empty string in the specific column
df['Comment'] = df['Comment'].str.replace(url_pattern, '', regex=True)


def normalize(sentence):

    sentence = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", sentence)
    sentence = re.sub("Ù‰", "ÙŠ", sentence)
    sentence = re.sub("Ø¤", "Ø¡", sentence)
    sentence = re.sub("Ø¦", "Ø¡", sentence)
    sentence = re.sub("Ø©", "Ù‡", sentence)
    sentence = re.sub("Ú¯", "Ùƒ", sentence)
    return sentence

def normalize_reviews(df, column_name):
    df[column_name ] = df[column_name].apply(lambda x: normalize(x) if isinstance(x, str) else x)
    return df


df = normalize_reviews(df, 'Comment')
df.head(10)

spec_chars = ["!",'"',"#","%","&","'","(",")",
              "*","+",",","-",".","/",":",";","<",
              "=",">","?","@","[","\\","]","^","_",
              "`","{","|","}","~","â€“", "ØŸ","\"\"","\"",'""']
for char in spec_chars:
    df['Comment'] = df['Comment'].str.replace(char, ' ')

df.head(10)

# Remove punctuations
df['Comment'] = df['Comment'].astype(str)
df['Comment'] = df['Comment'].apply(lambda x: re.sub('[%s]' % re.escape("""!"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\]^_`{|}~ \"|#|$|%|&|\'|\*|\+|,|-|\.|\/|:||;|<|=|>|[|\\|]|^|_|`|{||}~"""), ' ', x))
df['Comment'] = df['Comment'].apply(lambda x: re.sub(r'([@A-Za-z0-9_Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€]+)|[^\w\s]|#|http\S+', '', x))

pattern = r'(.)\1+'
df['Comment'] = df['Comment'].replace(pattern, r'\1', regex=True)
df.head(10)

#Remove arkam w stop words
def remove_numbers(text):
    return re.sub(r'\d+', '', str(text))


# Apply the remove_numbers function to the Comment column
df['Comment'] = df['Comment'].apply(remove_numbers)

nltk.download('stopwords')
nltk.download('punkt')

stop_words = stopwords.words('arabic')

def remove_stop_words(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    return ' '.join(filtered_tokens)

# Apply the remove_stop_words function to the "Comment" column
df['Comment'] = df['Comment'].apply(remove_stop_words)

df = df.dropna()


def remove_duplicate_words(text):
    # Check for None values
    if text is None:
        return None

    # Split the text into words
    words = text.split()

    # Remove duplicates while preserving the order
    unique_words = []
    for word in words:
        if word not in unique_words:
            unique_words.append(word)

    # Reconstruct the sentence with unique words
    cleaned_text = ' '.join(unique_words)

    return cleaned_text

# Remove rows with 'None' values in the 'Comment' column
df = df[df['Comment'].notna()]

# Apply the function to the 'Comment' column in your DataFrame
df['Comment'] = df['Comment'].apply(remove_duplicate_words)
# print(df.head(10))


# Remove ExtraChar like "Ø§Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù‡" to "Ø§Ù„Ù„Ù‡"
df['Comment'] = df['Comment'].apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))
df.Comment=df.Comment.apply(lambda x:x.replace('Ø›',"", ))
df.head(10)

#replace emojis with words

emojis = {
    "ğŸ™‚":" ÙŠØ¨ØªØ³Ù… ",
    "ğŸ˜‚":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ’”":" Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ† ",
    "ğŸ™‚":" ÙŠØ¨ØªØ³Ù… ",
    "â¤ï¸":" Ø­Ø¨ ",
    "â¤":" Ø­Ø¨ ",
    "ğŸ˜":" Ø­Ø¨ ",
    "ğŸ˜­":" ÙŠØ¨ÙƒÙŠ ",
    "ğŸ˜¢":" Ø­Ø²Ù† ",
    "ğŸ˜”":" Ø­Ø²Ù† ",
    "â™¥":" Ø­Ø¨ ",
    "ğŸ’œ":" Ø­Ø¨ ",
    "ğŸ˜…":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ™":" Ø­Ø²ÙŠÙ† ",
    "ğŸ’•":" Ø­Ø¨ ",
    "ğŸ’™":" Ø­Ø¨ ",
    "ğŸ˜":" Ø­Ø²ÙŠÙ† ",
    "ğŸ˜Š":" Ø³Ø¹Ø§Ø¯Ø© ",
    "ğŸ‘":" ÙŠØµÙÙ‚ ",
    "ğŸ‘Œ":" Ø§Ø­Ø³Ù†Øª ",
    "ğŸ˜´":" ÙŠÙ†Ø§Ù… ",
    "ğŸ˜€":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ˜Œ":" Ø­Ø²ÙŠÙ† ",
    "ğŸŒ¹":" ÙˆØ±Ø¯Ø© ",
    "ğŸ™ˆ":" Ø­Ø¨ ",
    "ğŸ˜„":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ˜":" Ù…Ø­Ø§ÙŠØ¯ ",
    "âœŒ":" Ù…Ù†ØªØµØ± ",
    "âœ¨":" Ù†Ø¬Ù…Ù‡ ",
    "ğŸ¤”":" ØªÙÙƒÙŠØ± ",
    "ğŸ˜":" ÙŠØ³ØªÙ‡Ø²Ø¡ ",
    "ğŸ˜’":" ÙŠØ³ØªÙ‡Ø²Ø¡ ",
    "ğŸ™„":" Ù…Ù„Ù„ ",
    "ğŸ˜•":" Ø¹ØµØ¨ÙŠØ© ",
    "ğŸ˜ƒ":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸŒ¸":" ÙˆØ±Ø¯Ø© ",
    "ğŸ˜“":" Ø­Ø²Ù† ",
    "ğŸ’":" Ø­Ø¨ ",
    "ğŸ’—":" Ø­Ø¨ ",
    "ğŸ˜‘":" Ù…Ù†Ø²Ø¹Ø¬ ",
    "ğŸ’­":" ØªÙÙƒÙŠØ± ",
    "ğŸ˜":" Ø«Ù‚Ø© ",
    "ğŸ’›":" Ø­Ø¨ ",
    "ğŸ˜©":" Ø­Ø²ÙŠÙ† ",
    "ğŸ’ª":" Ø¹Ø¶Ù„Ø§Øª ",
    "ğŸ‘":" Ù…ÙˆØ§ÙÙ‚ ",
    "ğŸ™ğŸ»":" Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨ ",
    "ğŸ˜³":" Ù…ØµØ¯ÙˆÙ… ",
    "ğŸ‘ğŸ¼":" ØªØµÙÙŠÙ‚ ",
    "ğŸ¶":" Ù…ÙˆØ³ÙŠÙ‚ÙŠ ",
    "ğŸŒš":" ØµÙ…Øª ",
    "ğŸ’š":" Ø­Ø¨ ",
    "ğŸ™":" Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨ ",
    "ğŸ’˜":" Ø­Ø¨ ",
    "ğŸƒ":" Ø³Ù„Ø§Ù… ",
    "â˜º":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ¸":" Ø¶ÙØ¯Ø¹ ",
    "ğŸ˜¶":" Ù…ØµØ¯ÙˆÙ… ",
    "âœŒï¸":" Ù…Ø±Ø­ ",
    "âœ‹ğŸ»":" ØªÙˆÙ‚Ù ",
    "ğŸ˜‰":" ØºÙ…Ø²Ø© ",
    "ğŸŒ·":" Ø­Ø¨ ",
    "ğŸ™ƒ":" Ù…Ø¨ØªØ³Ù… ",
    "ğŸ˜«":" Ø­Ø²ÙŠÙ† ",
    "ğŸ˜¨":" Ù…ØµØ¯ÙˆÙ… ",
    "ğŸ¼ ":" Ù…ÙˆØ³ÙŠÙ‚ÙŠ ",
    "ğŸ":" Ù…Ø±Ø­ ",
    "ğŸ‚":" Ù…Ø±Ø­ ",
    "ğŸ’Ÿ":" Ø­Ø¨ ",
    "ğŸ˜ª":" Ø­Ø²Ù† ",
    "ğŸ˜†":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ˜£":" Ø§Ø³ØªÙŠØ§Ø¡ ",
    "â˜ºï¸":" Ø­Ø¨ ",
    "ğŸ˜±":" ÙƒØ§Ø±Ø«Ø© ",
    "ğŸ˜":" ÙŠØ¶Ø­Ùƒ ",
    "ğŸ˜–":" Ø§Ø³ØªÙŠØ§Ø¡  ",
    "ğŸƒğŸ¼":" ÙŠØ¬Ø±ÙŠ ",
    "ğŸ˜¡":" ØºØ¶Ø¨ ",
    "ğŸš¶":" ÙŠØ³ÙŠØ± ",
    "ğŸ¤•":" Ù…Ø±Ø¶ ",
    "â€¼ï¸":" ØªØ¹Ø¬Ø¨ ",
    "ğŸ•Š":" Ø·Ø§Ø¦Ø± ",
    "ğŸ‘ŒğŸ»":" Ø§Ø­Ø³Ù†Øª ",
    "â£":" Ø­Ø¨ ",
    "ğŸ™Š":" Ù…ØµØ¯ÙˆÙ… ",
    "ğŸ’ƒ":" Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­ ",
    "ğŸ’ƒğŸ¼":" Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­ ",
    "ğŸ˜œ":" Ù…Ø±Ø­ ",
    "ğŸ‘Š":" Ø¶Ø±Ø¨Ø© ",
    "ğŸ˜Ÿ":" Ø§Ø³ØªÙŠØ§Ø¡ ",
    "ğŸ’–":" Ø­Ø¨ ",
    "ğŸ˜¥":" Ø­Ø²Ù† ",
    "ğŸ»":" Ù…ÙˆØ³ÙŠÙ‚ÙŠ ",
    "âœ’":" ÙŠÙƒØªØ¨ ",
    "ğŸš¶ğŸ»":" ÙŠØ³ÙŠØ± ",
    "ğŸ’":" Ø§Ù„Ù…Ø§Ø¸ ",
    "ğŸ˜·":" ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶ ",
    "â˜":" ÙˆØ§Ø­Ø¯ ",
    "ğŸš¬":" ØªØ¯Ø®ÙŠÙ† ",
    "ğŸ’" : " ÙˆØ±Ø¯ ",
    "ğŸŒ" : " Ø´Ù…Ø³ ",
    "ğŸ‘†" : " Ø§Ù„Ø§ÙˆÙ„ ",
    "âš ï¸" :" ØªØ­Ø°ÙŠØ± ",
    "ğŸ¤—" : " Ø§Ø­ØªÙˆØ§Ø¡ ",
    "âœ–ï¸": " ØºÙ„Ø· ",
    "ğŸ“"  : " Ù…ÙƒØ§Ù† ",
    "ğŸ‘¸" : " Ù…Ù„ÙƒÙ‡ ",
    "ğŸ‘‘" : " ØªØ§Ø¬ ",
    "âœ”ï¸" : " ØµØ­ ",
    "ğŸ’Œ": " Ù‚Ù„Ø¨ ",
    "ğŸ˜²" : " Ù…Ù†Ø¯Ù‡Ø´ ",
    "ğŸ’¦": " Ù…Ø§Ø¡ ",
    "ğŸš«" : " Ø®Ø·Ø§ ",
    "ğŸ‘ğŸ»" : " Ø¨Ø±Ø§ÙÙˆ ",
    "ğŸŠ" :" ÙŠØ³Ø¨Ø­ ",
    "ğŸ‘ğŸ»": " ØªÙ…Ø§Ù… ",
    "â­•ï¸" :" Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡ ",
    "ğŸ·" : " Ø³Ø§ÙƒØ³ÙÙˆÙ† ",
    "ğŸ‘‹": " ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯ ",
    "âœŒğŸ¼": " Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ± ",
    "ğŸŒ":" Ù…Ø¨ØªØ³Ù… ",
    "â¿"  : " Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡ ",
    "ğŸ’ªğŸ¼" : " Ù‚ÙˆÙŠ ",
    "ğŸ“©":  " ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ ",
    "â˜•ï¸": " Ù‚Ù‡ÙˆÙ‡ ",
    "ğŸ˜§" : " Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø© ",
    "ğŸ—¨": " Ø±Ø³Ø§Ù„Ø© ",
    "â—ï¸" :" ØªØ¹Ø¬Ø¨ ",
    "ğŸ™†ğŸ»": " Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡ ",
    "ğŸ‘¯" :" Ø§Ø®ÙˆØ§Øª ",
    "Â©" :  " Ø±Ù…Ø² ",
    "ğŸ‘µğŸ½" :" Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡ ",
    "ğŸ£": " ÙƒØªÙƒÙˆØª ",
    "ğŸ™Œ": " ØªØ´Ø¬ÙŠØ¹ ",
    "ğŸ™‡": " Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ ",
    "ğŸ‘ğŸ½":" Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡ ",
    "ğŸ‘ŒğŸ½": " Ø¨Ø§Ù„Ø¸Ø¨Ø· ",
    "â‰ï¸" : " Ø§Ø³ØªÙ†ÙƒØ§Ø± ",
    "âš½ï¸": " ÙƒÙˆØ±Ù‡ ",
    "ğŸ•¶" :" Ø­Ø¨ ",
    "ğŸˆ" :" Ø¨Ø§Ù„ÙˆÙ† ",
    "ğŸ€":    " ÙˆØ±Ø¯Ù‡ ",
    "ğŸ’µ":  " ÙÙ„ÙˆØ³ ",
    "ğŸ˜‹":  " Ø¬Ø§Ø¦Ø¹ ",
    "ğŸ˜›":  " ÙŠØºÙŠØ¸ ",
    "ğŸ˜ ":  " ØºØ§Ø¶Ø¨ ",
    "âœğŸ»":  " ÙŠÙƒØªØ¨ ",
    "ğŸŒ¾":  " Ø§Ø±Ø² ",
    "ğŸ‘£":  " Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ† ",
    "âŒ":" Ø±ÙØ¶ ",
    "ğŸŸ":" Ø·Ø¹Ø§Ù… ",
    "ğŸ‘¬":" ØµØ¯Ø§Ù‚Ø© ",
    "ğŸ°":" Ø§Ø±Ù†Ø¨ ",
    "â˜‚":" Ù…Ø·Ø± ",
    "âšœ":" Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§ ",
    "ğŸ‘":" Ø®Ø±ÙˆÙ ",
    "ğŸ—£":" ØµÙˆØª Ù…Ø±ØªÙØ¹ ",
    "ğŸ‘ŒğŸ¼":" Ø§Ø­Ø³Ù†Øª ",
    "â˜˜":" Ù…Ø±Ø­ ",
    "ğŸ˜®":" ØµØ¯Ù…Ø© ",
    "ğŸ˜¦":" Ù‚Ù„Ù‚ ",
    "â­•":" Ø§Ù„Ø­Ù‚ ",
    "âœï¸":" Ù‚Ù„Ù… " ,
    "â„¹":" Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ",
    "ğŸ™ğŸ»":" Ø±ÙØ¶ ",
    "âšªï¸":" Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡ ",
    "ğŸ¤":" Ø­Ø²Ù† ",
    "ğŸ’«":" Ù…Ø±Ø­ ",
    "ğŸ’":" Ø­Ø¨ ",
    "ğŸ”":" Ø·Ø¹Ø§Ù… ",
    "â¤ï¸":" Ø­Ø¨ ",
    "âœˆï¸":" Ø³ÙØ± ",
    "ğŸƒğŸ»â€â™€ï¸":" ÙŠØ³ÙŠØ± ",
    "ğŸ³":" Ø°ÙƒØ± ",
    "ğŸ¤":" Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡ ",
    "ğŸ¾":" ÙƒØ±Ù‡ ",
    "ğŸ”":" Ø¯Ø¬Ø§Ø¬Ø© ",
    "ğŸ™‹":" Ø³Ø¤Ø§Ù„ ",
    "ğŸ“®":" Ø¨Ø­Ø± ",
    "ğŸ’‰":" Ø¯ÙˆØ§Ø¡ ",
    "ğŸ™ğŸ¼":" Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨ ",
    "ğŸ’‚ğŸ¿ ":" Ø­Ø§Ø±Ø³ ",
    "ğŸ¬":" Ø³ÙŠÙ†Ù…Ø§ ",
    "â™¦ï¸":" Ù…Ø±Ø­ ",
    "ğŸ’¡":" Ù‚ÙƒØ±Ø© ",
    "â€¼":" ØªØ¹Ø¬Ø¨ ",
    "ğŸ‘¼":" Ø·ÙÙ„ ",
    "ğŸ”‘":" Ù…ÙØªØ§Ø­ ",
    "â™¥ï¸":" Ø­Ø¨ ",
    "ğŸ•‹":" ÙƒØ¹Ø¨Ø© ",
    "ğŸ“":" Ø¯Ø¬Ø§Ø¬Ø© ",
    "ğŸ’©":" Ù…Ø¹ØªØ±Ø¶ ",
    "ğŸ‘½":" ÙØ¶Ø§Ø¦ÙŠ ",
    "â˜”ï¸":" Ù…Ø·Ø± ",
    "ğŸ·":" Ø¹ØµÙŠØ± ",
    "ğŸŒŸ":" Ù†Ø¬Ù…Ø© ",
    "â˜ï¸":" Ø³Ø­Ø¨ ",
    "ğŸ‘ƒ":" Ù…Ø¹ØªØ±Ø¶ ",
    "ğŸŒº":" Ù…Ø±Ø­ ",
    "ğŸ”ª":" Ø³ÙƒÙŠÙ†Ø© ",
    "â™¨":" Ø³Ø®ÙˆÙ†ÙŠØ© ",
    "ğŸ‘ŠğŸ¼":" Ø¶Ø±Ø¨ ",
    "âœ":" Ù‚Ù„Ù… ",
    "ğŸš¶ğŸ¾â€â™€ï¸":" ÙŠØ³ÙŠØ± ",
    "ğŸ‘Š":" Ø¶Ø±Ø¨Ø© ",
    "â—¾ï¸":" ÙˆÙ‚Ù ",
    "ğŸ˜š":" Ø­Ø¨ ",
    "ğŸ”¸":" Ù…Ø±Ø­ ",
    "ğŸ‘ğŸ»":" Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ ",
    "ğŸ‘ŠğŸ½":" Ø¶Ø±Ø¨Ø© ",
    "ğŸ˜™":" Ø­Ø¨ ",
    "ğŸ¥":" ØªØµÙˆÙŠØ± ",
    "ğŸ‘‰":" Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡ ",
    "ğŸ‘ğŸ½":" ÙŠØµÙÙ‚ ",
    "ğŸ’ªğŸ»":" Ø¹Ø¶Ù„Ø§Øª ",
    "ğŸ´":" Ø§Ø³ÙˆØ¯ ",
    "ğŸ”¥":" Ø­Ø±ÙŠÙ‚ ",
    "ğŸ˜¬":" Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø© ",
    "ğŸ‘ŠğŸ¿":" ÙŠØ¶Ø±Ø¨ ",
    "ğŸŒ¿":" ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡ ",
    "âœ‹ğŸ¼":" ÙƒÙ Ø§ÙŠØ¯ ",
    "ğŸ‘":" Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡ ",
    "â˜ ï¸":" ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨ ",
    "ğŸ‰":" ÙŠÙ‡Ù†Ø¦ ",
    "ğŸ”•" :" ØµØ§Ù…Øª ",
    "ğŸ˜¿":" ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ† ",
    "â˜¹ï¸":" ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³ ",
    "ğŸ˜˜" :" Ø­Ø¨ ",
    "ğŸ˜°" :" Ø®ÙˆÙ Ùˆ Ø­Ø²Ù† ",
    "ğŸŒ¼":" ÙˆØ±Ø¯Ù‡ ",
    "ğŸ’‹":  " Ø¨ÙˆØ³Ù‡ ",
    "ğŸ‘‡":" Ù„Ø§Ø³ÙÙ„  ",
    "â£ï¸":" Ø­Ø¨ ",
    "ğŸ§":" Ø³Ù…Ø§Ø¹Ø§Øª ",
    "ğŸ“":" ÙŠÙƒØªØ¨ ",
    "ğŸ˜‡":" Ø¯Ø§ÙŠØ® ",
    "ğŸ˜ˆ":" Ø±Ø¹Ø¨ ",
    "ğŸƒ":" ÙŠØ¬Ø±ÙŠ ",
    "âœŒğŸ»":" Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ± ",
    "ğŸ”«":" ÙŠØ¶Ø±Ø¨ ",

    "â—ï¸":" ØªØ¹Ø¬Ø¨ ",
    "ğŸ‘":" ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚ ",
    "ğŸ”":" Ù‚ÙÙ„ ",
    "ğŸ‘ˆ":" Ù„Ù„ÙŠÙ…ÙŠÙ† ",
    "â„¢":" Ø±Ù…Ø² ",
    "ğŸš¶ğŸ½":" ÙŠØªÙ…Ø´ÙŠ ",
    "ğŸ˜¯":" Ù…ØªÙØ§Ø¬Ø£ ",
    "âœŠ":" ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡ ",
    "ğŸ˜»":" Ø§Ø¹Ø¬Ø§Ø¨ ",
    "ğŸ™‰" :" Ù‚Ø±Ø¯ ",
    "ğŸ‘§":" Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡ ",
    "ğŸ”´":" Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡ ",
    "ğŸ’ªğŸ½":" Ù‚ÙˆÙ‡ ",
    "ğŸ’¤":" ÙŠÙ†Ø§Ù… ",
    "ğŸ‘€":" ÙŠÙ†Ø¸Ø± ",
    "âœğŸ»":" ÙŠÙƒØªØ¨ ",
    "â„ï¸":" ØªÙ„Ø¬ ",
    "ğŸ’€":" Ø±Ø¹Ø¨ ",
    "ğŸ˜¤":" ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³ ",
    "ğŸ–‹":" Ù‚Ù„Ù… ",
    "ğŸ©":" ÙƒØ§Ø¨ ",
    "â˜•ï¸":" Ù‚Ù‡ÙˆÙ‡ ",
    "ğŸ˜¹":" Ø¶Ø­Ùƒ ",
    "ğŸ’“":" Ø­Ø¨  ",
    "â˜„ï¸ ":" Ù†Ø§Ø± ",
    "ğŸ‘»":" Ø±Ø¹Ø¨ ",
    "â":" Ø®Ø·Ø¡ ",
    "ğŸ¤®":" Ø­Ø²Ù† ",
    'ğŸ»':" Ø§Ø­Ù…Ø± ",
    "ğŸ’§":"Ø¯Ù…Ø¹Ø©",
    "ğŸŒ³":"Ø´Ø¬Ø±Ù‡",
    "ğŸ–¤" : "Ù‚Ù„Ø¨ Ø§Ø³ÙˆØ¯",
    "ğŸŒ‚" :"Ø´Ù…Ø³ÙŠÙ‡",
    "ğŸ¥ºâƒ¢" : " ÙƒÙŠÙˆØª",
    "â„" : " ØªÙ„Ø¬",
    "ğŸƒ" : "ÙŠÙ‚Ø·ÙŠÙ†Ø©",
    "ğŸ¤ª": "Ù…Ø¬Ù†ÙˆÙ†",
    "âœ… ": "Ø¹Ù„Ø§Ù…Ø© ØµØ­",
    "ğŸ‘°" : "Ø¹Ø±ÙˆØ³",
    "ğŸ˜—" : "ÙˆØ¬Ù‡ ÙŠÙ‚Ø¨Ù„",
    "ğŸ" : "Ù‡Ø¯ÙŠØ©" ,
    "ğŸ¤" : "Ù…ØµØ§ÙØ­Ø©",
    "ğŸ’¯" : "Ù…Ø¦Ø© ÙÙŠ Ø§Ù„Ù…Ø¦Ø©",
    "ğŸ¤¯" :"Ø°Ù‡ÙˆÙ„" ,
    "ğŸ¦›" : "ÙØ±Ø³ Ø§Ù„Ù†Ù‡Ø±",
    "ğŸ¦„ ": "Ø­ØµØ§Ù† Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚Ø±Ù†",
    "ğŸ¦“ ":"Ø­Ù…Ø§Ø± ÙˆØ­Ø´ÙŠ",
    "ğŸ´" :"Ø­ØµØ§Ù†",
    "ğŸ¥° ":"ÙˆØ¬Ù‡ Ø¨Ø¹ÙŠÙˆÙ† Ø§Ù„Ù‚Ù„Ø¨",
    "ğŸ‘…" : "Ù„Ø³Ø§Ù†",
    "ğŸ‘„" : "Ø´ÙØ§Ù‡",
    "ğŸ¤´" : "Ø£Ù…ÙŠØ±",
    "ğŸš® ": "Ø³Ù„Ø© Ù…Ù‡Ù…Ù„Ø§Øª",
    "ğŸ¤© ":"ÙˆØ¬Ù‡ Ø¨Ø¹ÙŠÙˆÙ† Ù†Ø¬Ù…ÙŠØ©",
    "ğŸ›‘ ": "Ø¥Ø´Ø§Ø±Ø© ØªÙˆÙ‚Ù",
    "âœ‹ ":"ÙŠØ¯ Ù…Ø±ÙÙˆØ¹Ø©",
}

emoticons_to_emoji = {
    ":)" : "ğŸ™‚",
    ":(" : "ğŸ™",
    "xD" : "ğŸ˜†",
    ":=(": "ğŸ˜­",
    ":'(": "ğŸ˜¢",
    ":'â€‘(": "ğŸ˜¢",
    "XD" : "ğŸ˜‚",
    ":D" : "ğŸ™‚",
    "â™¬" : "Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "â™¡" : "â¤",
    "â˜»"  : "ğŸ™‚",
}
def replace_emojis_emoticons(text):
    # Check if text is None
    if text is None:
        return text  # or return '' if you prefer to replace None with an empty string

    # Replace emoticons with emojis
    for emoticon, emoji in emoticons_to_emoji.items():
        text = text.replace(emoticon, emoji)

    # Replace emojis with words
    for emoji, word in emojis.items():
        text = text.replace(emoji, word)

    return text

df['Comment'] = df['Comment'].apply(replace_emojis_emoticons)
df.head(5)
df = df.dropna()
df = df.replace('', pd.NA)
df = df.dropna()
# print(df.head(10))
df.to_csv('comments.csv', index=False, encoding='utf-8-sig')
print("*******************************************************")
print("!!!!!!!!!!!!!! Done!!!!!!!!!!!!!!!!!")
print("*******************************************************")
